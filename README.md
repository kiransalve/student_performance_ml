
# Student Permormance ML Project - Prediction of Maths score

This project taken from Krish Sir's youtube video - https://www.youtube.com/watch?v=NuwUnRpxq2c&list=PLTDARY42LDV7jzL_f68SY-eOQ9tY2lYvR&index=1

We have students data with 

cat_cols = ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']

num_cols = ['reading_score', 'writing_score', "math_score"]

## Traning Pipeline

-> Getting data

-> Train Test Split

-> Exploratory data analysis

-> Feature Transformation

-> Model Training, Evalution, Monitoring

Components - 

1. Data Source - MySQL, MongoDB - Collecting data from various sources to one point

2. Data Ingestion - We read data from above Data Source and do Train Test Split

3. Data Transformation - EDA and Feature Engineering

4. Model Trainer - Multiple model - will choose best one

5. Model Monitoring - (Evidently AI) CI CD Pipelines using Github Actions

6. Model Deployment 


## Prediction Pipeline 

We give input and this give output


## ğŸ§Š Data Ingestion Process 

### Function Explaination

```text
app.py
â”‚
â””â”€â”€> DataIngestion().initiate_data_ingestion()
     â”‚
     â”œâ”€â”€ Calls `read_sql_data()`
     â”‚     â””â”€â”€ Fetches raw data from MySQL database into DataFrame
     â”‚
     â”œâ”€â”€ Creates directories if not exist
     â”‚
     â”œâ”€â”€ Saves raw data as CSV
     â”‚     â””â”€â”€ `artifacts/raw.csv`
     â”‚
     â”œâ”€â”€ Splits data into train and test (80/20)
     â”‚
     â”œâ”€â”€ Saves train data
     â”‚     â””â”€â”€ `artifacts/train.csv`
     â”‚
     â”œâ”€â”€ Saves test data
     â”‚     â””â”€â”€ `artifacts/test.csv`
     â”‚
     â””â”€â”€ Returns train and test file paths
```

## Flow Explaination

When we run the app, this is what happens in the **DataIngestion** step:

```text
app.py  
â”‚  
â””â”€â”€> Starts `DataIngestion().initiate_data_ingestion()`
     â”‚
     â”œâ”€â”€ âœ… Reads data from a MySQL database  
     â”‚      (This is like getting the records from MySQL Workbench)
     â”‚
     â”œâ”€â”€ ğŸ—‚ï¸ Creates a folder to store files (if it doesnâ€™t already exist)
     â”‚
     â”œâ”€â”€ ğŸ“„ Saves the full original data into a file called `raw.csv`
     â”‚      (So we have a backup of the complete data)
     â”‚
     â”œâ”€â”€ âœ‚ï¸ Splits the data into two parts:
     â”‚      - Train data (80%) â€“ used to train the model  
     â”‚      - Test data (20%) â€“ used to check how well the model performs  
     â”‚
     â”œâ”€â”€ ğŸ’¾ Saves the train data in `train.csv`  
     â”œâ”€â”€ ğŸ’¾ Saves the test data in `test.csv`  
     â”‚
     â””â”€â”€ ğŸ” Returns the paths of these two files so the next step can use them

```

## Function Explaination 

### ğŸ”„ Data Transformation Flow (from `app.py`)

```text
app.py
 â”‚
 â””â”€â”€> DataTransformation().initiate_data_transformation(train_path, test_path)
       â”‚
       â”œâ”€â”€ Reads train & test CSVs as DataFrames
       â”‚
       â”œâ”€â”€ Calls get_data_transformation_object()
       â”‚     â””â”€â”€ Builds and returns the ColumnTransformer pipeline
       â”‚
       â”œâ”€â”€ Splits train_df & test_df into:
       â”‚     - input_features (X)
       â”‚     - target (y)
       â”‚
       â”œâ”€â”€ Applies .fit_transform() on train X
       â”‚
       â”œâ”€â”€ Applies .transform() on test X
       â”‚
       â”œâ”€â”€ Combines X and y into numpy arrays:
       â”‚     - train_arr
       â”‚     - test_arr
       â”‚
       â”œâ”€â”€ Saves the preprocessor pipeline object as `preprocessor.pkl`
       â”‚
       â””â”€â”€ Returns train_arr, test_arr, path_to_preprocessor.pkl

```

## Flow Exaplianation

### ğŸ“Š Data Transformation Flow

```text
app.py
 â”‚
 â””â”€â”€> DataTransformation().initiate_data_transformation(train_path, test_path)
       â”‚
       â”œâ”€â”€ Loads the training and test data files (like Excel or CSV)
       â”‚
       â”œâ”€â”€ Prepares a pipeline that:
       â”‚     â””â”€â”€ Cleans the data (fills blanks, fixes formats)
       â”‚     â””â”€â”€ Converts categorical features into numbers as ML only understand numbers
       â”‚     â””â”€â”€ Scales numbers at common 
       â”‚
       â”œâ”€â”€ Separates both files into:
       â”‚     - Input data (what we use to make predictions)
       â”‚     - Output data (what we want to predict)
       â”‚
       â”œâ”€â”€ Learns from the training input data
       â”‚
       â”œâ”€â”€ Uses the pipeline to prepare the test input data
       â”‚
       â”œâ”€â”€ Combines the inputs and outputs into:
       â”‚     - Final training set
       â”‚     - Final test set
       â”‚
       â”œâ”€â”€ Saves it as "preprocessor" in a file so it can be reused
       â”‚
       â””â”€â”€ Gives back:
             - The final training data
             - The final test data
             - The path where the pkl file was saved (preprocessor.pkl)
```

### ğŸ¤– Model Training Flow 

```text
app.py
 â”‚
 â””â”€â”€> ModelTrainer().initiate_model_trainer(train_array, test_array)
       â”‚
       â”œâ”€â”€ Splits both arrays into:
       â”‚     - X_train (features to learn from)
       â”‚     - y_train (target values for training)
       â”‚     - X_test  (features to test with)
       â”‚     - y_test  (real target values for test)
       â”‚
       â”œâ”€â”€ Defines a set of machine learning models:
       â”‚     - Random Forest, Decision Tree, Gradient Boosting, etc.
       â”‚
       â”œâ”€â”€ Provides tuning options (hyperparameters) for each model
       â”‚
       â”œâ”€â”€ Calls evaluate_models() to:
       â”‚     â””â”€â”€ Train each model with tuning options
       â”‚     â””â”€â”€ Test each model
       â”‚     â””â”€â”€ Return performance scores
       â”‚
       â”œâ”€â”€ Selects the best-performing model
       â”‚
       â”œâ”€â”€ Checks if best model's score is good enough
       â”‚     â””â”€â”€ If not, raises an error
       â”‚
       â”œâ”€â”€ Saves the best model as a file (for future use)
       â”‚
       â”œâ”€â”€ Uses best model to predict on test data
       â”‚
       â””â”€â”€ Returns the modelâ€™s accuracy score (RÂ² score)
```

## ğŸ¤– Model Trainer Process (Easy Explanation)

This step chooses the best machine learning model from many options by trying them all and picking the one that performs best.

```text
app.py  
â”‚  
â””â”€â”€> Starts `ModelTrainer().initiate_model_trainer(train_array, test_array)`
     â”‚
     â”œâ”€â”€ ğŸ”€ Splits the data:
     â”‚      - X = inputs (like features: sales, zone, product)
     â”‚      - y = output (what we want to predict)
     â”‚
     â”œâ”€â”€ ğŸ“¦ Prepares a list of models to try:
     â”‚      - Random Forest
     â”‚      - Decision Tree
     â”‚      - Gradient Boosting
     â”‚      - Linear Regression
     â”‚      - XGBoost
     â”‚      - CatBoost
     â”‚      - AdaBoost
     â”‚
     â”œâ”€â”€ ğŸ› ï¸ Defines parameters to test for each model
     â”‚      (like how deep a tree should be, how fast to learn, etc.)
     â”‚
     â”œâ”€â”€ ğŸ“Š Tries every model and tracks performance using RÂ² score
     â”‚      - RÂ² tells how well predictions match actual results (closer to 1 is better)
     â”‚
     â”œâ”€â”€ ğŸ† Finds the best model
     â”‚      - If no model is good enough (RÂ² < 0.6), it raises an error
     â”‚
     â”œâ”€â”€ ğŸ’¾ Saves the best model in a file: `artifact/model.pkl`
     â”‚      (This file is used later for predictions)
     â”‚
     â””â”€â”€ ğŸ” Returns the RÂ² score of the best model
```
